# Combining Predictions for Accurate Recommender Systems

Este *paper* de los autores Jahrer, Töscher y Legenstein, trata sobre la combinación lineal de algoritmos de filtrado colaborativo, con el objetivo de superar el rendimiento de cada algoritmo funcionando por separado. Se ven ejemplos de algoritmos que se pueden combinar, y métodos para realizar estas combinaciones. Como en muchas otras publicaciones relacionadas a sistemas recomendadores, se utiliza el *dataset* Netflix Prize para mostrar y comparar resultados.

Luego de introducir el *paper*, comienzan con una sección en la que van mencionando los principales algoritmos que mejores resultados han dado por sí solos con el Netfliz Prize *dataset*. Estos son explicados de manera simple pero fiel, a modo de dar una idea introductoria sobre cómo funciona el algoritmo, o bien, ayudar a recordar al lector cuáles eran las particularidades de cada uno. En mi caso, esta parte me fue muy útil para aprender o recordar sobre estos algoritmos sin la necesidad de acudir a otros *papers* leídos previamente.

Luego, en la sección de *blending*, se ven los métodos que proponen para realizar las combinaciones de algoritmos, y las etapas necesarias de la combinación, como la selección de parámetros. En cuanto a esto, me pareció curioso que hayan extraído un *probe set* desde el set grande de *train*. Esto, según entendí, les ayudó a reducir el costo para la obtención de parámetros, lo que de otra forma hubiera resultado innecesariamente caro. Creo que, si bien no sabrán realmente si fueron elegidos los parámetros más adecuados, fue una buena decisión. Esto debido a que quizás aquellos parámetros no hubiesen sufrido un gran cambio al obtenerlos con el *training set* completo, lo que hubiese resultado en un costo extremadamente alto.

Más adelante, dentro de los métodos de *blending*, me llamó bastante la atención uno de ellos, el cual encontré altamente ingenioso. Este fue el de *Bagged Gradient Boosted Decision Tree* (BGBDT). Si bien no es el que finalmente recomiendan, me gustó la forma de entrenar recursivamente a un árbol simple. Se ve realmente complejo, pero en el pequeño fragmento que le pueden dedicar en el *paper*, lo logran explicar de muy buena manera.

Finalmente, el método con mejor rendimiento para combinar algoritmos es la red neuronal. Sin embargo, para aplicaciones prácticas recomiendan utilizarlo con *bagging*, ya que la velocidad de predicción puede aumentar considerablemente. Detalles como esta recomendación son valiosos, ya que no se quedan con un solo método, sino que también buscan la forma de optimizarlo, y entregan al lector resultados aún más convenientes.

